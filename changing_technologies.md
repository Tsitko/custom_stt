

# Локальный Orpheus TTS (rus) с клонированием голоса

Этот документ описывает запуск дообученного под русский язык Orpheus‑3B и базовый паттерн, как передавать пример голоса и текст в модель для синтеза в том же голосе.

## Цели

- Работать **локально** (Windows / WSL, 16 GB VRAM, 32 GB RAM).
- Использовать **дообученный под русский язык Orpheus**.
- Делать **zero‑shot voice cloning** по короткому эталонному аудио (5–15 сек).
- Оставить возможность:
    - добавлять управление эмоциями / стилем через текстовые теги;
    - обернуть всё в HTTP‑сервер (FastAPI) и/или UI (Gradio).

***

## 1. Выбор модели

### 1.1. Рекомендуемый чекпоинт

- Модель: `papacliff/orpheus-3b-0.1-ft-ru`.
- База: Orpheus‑3B, дообученный на ~188 часах русской речи (много дикторов, разные голоса).
- Особенности:
    - ориентирован на **естественную русскую просодию и интонацию**;
    - поддерживает **условие по аудио‑референсу** (пример голоса);
    - заточен под **LLM‑подобный интерфейс** — работает с последовательностью пар `текст + аудио`.

*(Точный README и параметры берутся с Hugging Face у самой модели.)*

***

## 2. Общая архитектура

### 2.1. Компоненты

- **Ядро TTS**: Orpheus‑3B русскоязычный финетюн.
- **Аудио‑кодек (SNAC или аналог)**: кодирует wav → дискретные токены, декодирует токены → wav.
- **Обвязка**:
    - Python‑скрипт для:
        - загрузки модели и кодека;
        - кодирования референсов;
        - генерации нового аудио.
    - (опционально) HTTP‑слой: FastAPI для REST API.
    - (опционально) UI: Gradio.


### 2.2. Логика использования

1. Пользователь даёт 1–N **референсных фраз**:
    - WAV с голосом (5–15 сек одной чистой фразы/нескольких фраз).
    - Транскрипт того, что сказано (на русском).
2. Приложение:
    - кодирует звук в аудио‑токены через кодек;
    - формирует «историю» из пар `(text, audio_codes)` и добавляет в конец `target_text`.
3. Orpheus генерирует аудио‑токены для `target_text` **в стиле и голосе** эталона.
4. Кодек декодирует токены в wav → возвращаем пользователю.

***

## 3. Установка (черновой план)

> Примечание: точные названия пакетов/классов зависят от конкретной реализации Orpheus‑обвязки (например, `orpheus-speech` или аналогичный пакет из README модели).

### 3.1. Базовая среда

```bash
# (WSL или Linux, либо Windows с Python 3.10+)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install --upgrade pip
pip install torch transformers soundfile
# Пакет обвязки Orpheus:
pip install orpheus-speech  # имя условное — свериться с README модели
```


***

## 4. Работа с эталонным голосом

### 4.1. Подготовка эталона

Требования к эталону:

- 5–15 секунд **чистой речи одного спикера** (без музыки, шумов, чужих голосов).
- Желательно стандартный sample rate (часто 24 kHz — зависит от кодека).
- Обязательно иметь **точный транскрипт** сказанного (для conditioning).

Пример файлов:

- `refs/my_voice_01.wav`
- `refs/my_voice_01.txt` (одна строка — текст того, что сказано в wav)


### 4.2. Код: загрузка модели и кодека

```python
import torch
import soundfile as sf

from orpheus_speech import OrpheusTTS, SnacCodec  # имена условны, сверить с реальной библиотекой

device = "cuda" if torch.cuda.is_available() else "cpu"

MODEL_NAME = "papacliff/orpheus-3b-0.1-ft-ru"

# Загрузка модели
model = OrpheusTTS.from_pretrained(MODEL_NAME).to(device)
model.eval()

# Загрузка аудио-кодека
codec = SnacCodec.from_pretrained(MODEL_NAME).to(device)
codec.eval()
```


### 4.3. Функция кодирования референса

```python
def encode_reference(wav_path: str):
    """
    wav_path: путь к эталонному WAV с голосом спикера.
    Возвращает тензор/структуру аудиокодов для Orpheus.
    """
    audio, sr = sf.read(wav_path)
    audio = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(device)  # [1, T]

    # При необходимости: ресемплинг до нужной частоты (например, 24kHz)
    # TODO: добавить ресемплер, если sr != 24000

    with torch.inference_mode():
        codes = codec.encode(audio)  # структура с аудио-токенами
    return codes
```


***

## 5. Генерация речи «в том же голосе»

### 5.1. Формат conditioning

Концептуально собираем список шагов:

- `(ref_text, ref_codes)` — пара «этот текст звучит вот так»;
- `(target_text, None)` — новый текст, для которого нужно сгенерировать звук.

Структура `conditioning` может выглядеть, например, так:

```python
conditioning = [
    {
        "text": "Привет, это мой эталонный пример голоса.",
        "audio_codes": ref_codes,
    },
    {
        "text": "<emotion:happy>Новый текст, который нужно произнести.",
        "audio_codes": None,
    }
]
```

Теги (`<emotion:happy>` и т.п.) — это просто часть текста, которые модель учится интерпретировать как стиль/эмоцию (если так её обучали).

### 5.2. Функция генерации

```python
def synth_in_ref_voice(
    target_text: str,
    ref_wav_path: str,
    ref_transcript: str,
    emotion: str = "neutral",
    sample_rate: int = 24000,
    out_path: str = "output_orpheus.wav",
):
    """
    Синтезирует target_text в голосе из ref_wav_path.
    ref_transcript — текст того, что сказано в ref_wav_path.
    emotion — строковый тег для стиля/эмоции (если модель его поддерживает).
    """

    # 1) Кодируем эталон
    ref_codes = encode_reference(ref_wav_path)

    # 2) Собираем conditioning
    emotion_prefix = f"<emotion:{emotion}>" if emotion else ""
    conditioning = [
        {
            "text": ref_transcript,
            "audio_codes": ref_codes,
        },
        {
            "text": emotion_prefix + target_text,
            "audio_codes": None,
        },
    ]

    # 3) Генерация аудио-кодов для целевого текста
    with torch.inference_mode():
        gen_codes = model.generate(conditioning)
        audio_hat = codec.decode(gen_codes)   # [1, T] float32

    # 4) Сохраняем файл
    audio_np = audio_hat[0].cpu().numpy()
    sf.write(out_path, audio_np, sample_rate)
    return out_path
```

Пример вызова:

```python
if __name__ == "__main__":
    out = synth_in_ref_voice(
        target_text="Привет, это пример синтеза речи в твоём голосе.",
        ref_wav_path="refs/my_voice_01.wav",
        ref_transcript="Привет, это мой эталонный пример голоса.",
        emotion="happy",
        sample_rate=24000,
        out_path="out_example.wav",
    )
    print("Готово:", out)
```


***

## 6. Идеи для HTTP‑API (набросок)

*(это только схема, не полный код)*

### 6.1. POST `/tts/clone`

Вход (JSON):

```json
{
  "references": [
    {
      "text": "Привет, это мой эталонный пример голоса.",
      "audio_base64": "<base64(wav)>"
    }
  ],
  "text": "Новый текст, который надо произнести.",
  "emotion": "happy",
  "sample_rate": 24000
}
```

Логика обработчика:

1. Декодировать `audio_base64` → WAV.
2. Для каждого референса вызвать `encode_reference(...)`.
3. Собрать conditioning: несколько `(text_i, codes_i)` и `(target_text, None)`.
4. Вызвать `model.generate(conditioning)`, `codec.decode(...)`.
5. Результат отправить как:
    - либо wav‑байты (или base64),
    - либо ссылку на сохранённый файл.

***

## 7. Практические рекомендации

- **Длина и качество референса**:
    - Лучше 6–15 секунд чистой речи (без шума и перебивок).
    - Можно использовать несколько примеров, тогда клонирование стабильнее.
- **Текст референса**:
    - Должен соответствовать содержимому звука;
    - Если нет точной транскрипции — лучше сначала прогнать эталон через STT и подправить.
- **Эмоции и стиль**:
    - Использовать текстовые теги (`<emotion:happy>`, `<style:news>` и т.п.), если модель их поддерживает.
    - Это всё можно потом генерировать LLM‑ом (например, отдельной функцией: «оберни фразу тегами для более злого/мягкого тона»).
- **Ресурсы**:
    - 3B‑модель + кодек укладываются в 16 GB VRAM (особенно в 8‑битном/квантованном варианте).
    - Можно сделать CPU‑fallback, если GPU занят (но медленнее).

***

## 8. Дальнейшее развитие

- Добавить:
    - FastAPI‑сервер (`/tts/clone`, `/health`, `/voices` и т.п.);
    - Gradio‑UI для локального тестирования.
- Придумать и стандартизировать собственный формат:
    - эмоций (`<emotion:angry>`, `<emotion:calm>`);
    - стилей (`<style:audiobook>`, `<style:blog>`).
- Подружить всё это с локальным LLM для:
    - предобработки текста (упрощение/нормализация);
    - генерации подходящих тегов стиля/эмоций.

***

Этот файл задуман как стартовый технический дизайн: дальше можно дописать реальные импорты/классы из конкретного Python‑пакета Orpheus, добавить реальные примеры из README модели и интеграцию с вашим стеком (FastAPI/WSL/Docker).

